{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5419bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\flipkart-mlflow-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLflow initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\flipkart-mlflow-env\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "\n",
    "mlflow.set_experiment(\"Flipkart_Sentiment_Model_Comparison\")\n",
    "\n",
    "print(\"✅ MLflow initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9bed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "print(\"✅ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0112bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (8518, 8)\n",
      "['Reviewer Name', 'Review Title', 'Place of Review', 'Up Votes', 'Down Votes', 'Month', 'Review text', 'Ratings']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b68a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after preparation:\n",
      "['Place of Review', 'Up Votes', 'Down Votes', 'Ratings', 'full_review']\n",
      "Dataset size after preparation: (8510, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2224\\343123678.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Review Title\"].fillna(\"\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2224\\343123678.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Place of Review\"].fillna(\"Unknown\", inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2224\\343123678.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Up Votes\"].fillna(0, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2224\\343123678.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Down Votes\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Drop Reviewer Name\n",
    "df.drop(columns=[\"Reviewer Name\"], inplace=True)\n",
    "\n",
    "# Handle Review Title\n",
    "df[\"Review Title\"].fillna(\"\", inplace=True)\n",
    "\n",
    "# Drop rows with missing Review text\n",
    "df = df.dropna(subset=[\"Review text\"])\n",
    "\n",
    "# Combine Review Title + Review text\n",
    "df[\"full_review\"] = (df[\"Review Title\"] + \" \" + df[\"Review text\"]).str.strip()\n",
    "\n",
    "# Handle Place of Review\n",
    "df[\"Place of Review\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Handle votes\n",
    "df[\"Up Votes\"].fillna(0, inplace=True)\n",
    "df[\"Down Votes\"].fillna(0, inplace=True)\n",
    "\n",
    "# Drop Month column\n",
    "df.drop(columns=[\"Month\"], inplace=True)\n",
    "\n",
    "# Drop original text columns\n",
    "df.drop(columns=[\"Review Title\", \"Review text\"], inplace=True)\n",
    "\n",
    "print(\"Columns after preparation:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"Dataset size after preparation:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b35c3fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary dataset size: 7895\n",
      "sentiment\n",
      "1    6823\n",
      "0    1072\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    elif rating >= 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df[\"sentiment\"] = df[\"Ratings\"].apply(create_sentiment)\n",
    "\n",
    "df_binary = df[df[\"sentiment\"] != 2].copy()\n",
    "\n",
    "print(\"Binary dataset size:\", len(df_binary))\n",
    "print(df_binary[\"sentiment\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0d74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing functions ready\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"read more\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, use_lemmatization=True):\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "\n",
    "    if use_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    else:\n",
    "        tokens = [stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"✅ Preprocessing functions ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11175dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after removing empty reviews: 7895\n"
     ]
    }
   ],
   "source": [
    "df_binary[\"cleaned_review\"] = df_binary[\"full_review\"].apply(\n",
    "    lambda x: preprocess_text(x, use_lemmatization=True)\n",
    ")\n",
    "\n",
    "df_binary = df_binary[df_binary[\"cleaned_review\"].str.len() > 0]\n",
    "\n",
    "print(\"Dataset size after removing empty reviews:\", len(df_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a42877a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6316\n",
      "Test size: 1579\n"
     ]
    }
   ],
   "source": [
    "X = df_binary[\"cleaned_review\"]\n",
    "y = df_binary[\"sentiment\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a191a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, vectorizer, model_name, feature_type, params):\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_{feature_type}\"):\n",
    "\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"feature\", feature_type)\n",
    "\n",
    "        for k, v in params.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # ✅ LOG ARTIFACT\n",
    "        os.makedirs(\"artifacts\", exist_ok=True)\n",
    "        joblib.dump(vectorizer, \"artifacts/vectorizer.pkl\")\n",
    "        mlflow.log_artifact(\"artifacts/vectorizer.pkl\")\n",
    "\n",
    "        print(f\"{model_name:<18} {feature_type:<6} \"\n",
    "              f\"Accuracy={acc:.6f} | F1={f1:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d889f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression BoW    Accuracy=0.925269 | F1=0.957736\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    CountVectorizer(max_features=5000, ngram_range=(1,2)),\n",
    "    \"Logistic Regression\", \"BoW\",\n",
    "    {\"max_features\": 5000}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5864094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression BoW    Accuracy=0.925269 | F1=0.957736\n",
      "SVM                TF-IDF Accuracy=0.920203 | F1=0.954709\n",
      "Naive Bayes        BoW    Accuracy=0.915136 | F1=0.952279\n",
      "XGBoost            TF-IDF Accuracy=0.917669 | F1=0.953505\n",
      "Random Forest      TF-IDF Accuracy=0.916403 | F1=0.952790\n",
      "Logistic Regression TF-IDF Accuracy=0.918303 | F1=0.954272\n",
      "Random Forest      BoW    Accuracy=0.913870 | F1=0.951498\n",
      "Naive Bayes        TF-IDF Accuracy=0.887904 | F1=0.938902\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic Regression + BoW\n",
    "run_experiment(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    CountVectorizer(max_features=5000, ngram_range=(1,2)),\n",
    "    \"Logistic Regression\", \"BoW\",\n",
    "    {\"max_features\": 5000}\n",
    ")\n",
    "\n",
    "# 2. SVM + TF-IDF\n",
    "run_experiment(\n",
    "    LinearSVC(C=1.0),\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    \"SVM\", \"TF-IDF\",\n",
    "    {\"max_features\": 5000, \"C\": 1.0}\n",
    ")\n",
    "\n",
    "# 3. Naive Bayes + BoW\n",
    "run_experiment(\n",
    "    MultinomialNB(),\n",
    "    CountVectorizer(max_features=5000),\n",
    "    \"Naive Bayes\", \"BoW\",\n",
    "    {\"max_features\": 5000}\n",
    ")\n",
    "\n",
    "# 4. XGBoost + TF-IDF\n",
    "run_experiment(\n",
    "    XGBClassifier(n_estimators=100, random_state=42, eval_metric=\"logloss\"),\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    \"XGBoost\", \"TF-IDF\",\n",
    "    {\"n_estimators\": 100}\n",
    ")\n",
    "\n",
    "# 5. Random Forest + TF-IDF\n",
    "run_experiment(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    \"Random Forest\", \"TF-IDF\",\n",
    "    {\"n_estimators\": 100}\n",
    ")\n",
    "\n",
    "# 6. Logistic Regression + TF-IDF\n",
    "run_experiment(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    TfidfVectorizer(max_features=5000, ngram_range=(1,2)),\n",
    "    \"Logistic Regression\", \"TF-IDF\",\n",
    "    {\"max_features\": 5000}\n",
    ")\n",
    "\n",
    "# 7. Random Forest + BoW\n",
    "run_experiment(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    CountVectorizer(max_features=5000),\n",
    "    \"Random Forest\", \"BoW\",\n",
    "    {\"n_estimators\": 100}\n",
    ")\n",
    "\n",
    "# 8. Naive Bayes + TF-IDF\n",
    "run_experiment(\n",
    "    MultinomialNB(),\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    \"Naive Bayes\", \"TF-IDF\",\n",
    "    {\"max_features\": 5000}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e9f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: file:../mlruns\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "print(\"Tracking URI:\", mlflow.get_tracking_uri())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb2a6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 11:33:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "e:\\anaconda3\\envs\\flipkart-mlflow-env\\Lib\\site-packages\\mlflow\\models\\model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
      "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
      "e:\\anaconda3\\envs\\flipkart-mlflow-env\\Lib\\site-packages\\mlflow\\tracking\\_model_registry\\utils.py:216: FutureWarning: The filesystem model registry backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri)\n",
      "Registered model 'Flipkart_Sentiment_Model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'Flipkart_Sentiment_Model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "with mlflow.start_run(run_name=\"LogReg_BoW_Register\"):\n",
    "\n",
    "    # Vectorizer\n",
    "    bow = CountVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "    X_train_bow = bow.fit_transform(X_train)\n",
    "    X_test_bow = bow.transform(X_test)\n",
    "\n",
    "    # Train BEST model\n",
    "    lr_bow = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = lr_bow.predict(X_test_bow)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    mlflow.log_param(\"feature\", \"BoW\")\n",
    "    mlflow.log_param(\"max_features\", 5000)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # ✅ REGISTER MODEL\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=lr_bow,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"Flipkart_Sentiment_Model\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cace2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features=1000 → F1=0.9580\n",
      "max_features=3000 → F1=0.9588\n",
      "max_features=5000 → F1=0.9577\n",
      "max_features=8000 → F1=0.9593\n"
     ]
    }
   ],
   "source": [
    "# STEP 4.1 – Hyperparameter sweep for max_features (ONE model only)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import mlflow\n",
    "\n",
    "max_features_list = [1000, 3000, 5000, 8000]\n",
    "\n",
    "for mf in max_features_list:\n",
    "    with mlflow.start_run(run_name=f\"LogReg_BoW_maxfeat_{mf}\"):\n",
    "\n",
    "        vectorizer = CountVectorizer(max_features=mf, ngram_range=(1,2))\n",
    "        X_train_vec = vectorizer.fit_transform(X_train)\n",
    "        X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model.fit(X_train_vec, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Log ONLY what matters for tuning\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"feature\", \"BoW\")\n",
    "        mlflow.log_param(\"max_features\", mf)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        print(f\"max_features={mf} → F1={f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flipkart-mlflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
